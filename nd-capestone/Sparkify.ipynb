{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#intro)<br>\n",
    "2. [Load & clean dataset](#wrangling)<br>\n",
    "3. [Exploratory data analysis](#eda)<br>\n",
    "4. [Feature engineering](#feateng)<br>\n",
    "5. [Modeling](#model)<br>\n",
    "6. [Conclusion](#results)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"intro\">1. Introduction</a>\n",
    "\n",
    "Along this project, we will be working on a challenging real case problem that many customer-facing business have to deal with, which is how to predict the churn rate of their users.\n",
    "\n",
    "To accomplish the project, we have been provided with a datastet from Sparkify organization.\n",
    "\n",
    "We will be working on large dataset hence usage of Spark will be key to manipulate the data. Spark ML lib will also be deterministic in the best choice of algorithm to pick and evaluate for our predictive model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdatetime\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkConf\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import datetime\n",
    "import time\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col,isnan, when, count,lower,udf, datediff,countDistinct\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Normalizer, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, LinearSVC, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SparkifyProject\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"wrangling\">2. Load & clean dataset</a>\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = spark.read.json(\"mini_sparkify_event_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for schema data types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of dataset\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for empty/null values\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.` We have some nan/null values that we'll have to take care of before building our model.\n",
    "Those empty values can not be dropped arbitrarily in our case since they might be expected for some of the events logged in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates rows cross all cols\n",
    "df.exceptAll(df.dropDuplicates(df.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check num of rows after removing dups \n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.` no duplicate row found checking for all cols in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case all col names\n",
    "df_clean = df.select([F.col(x).alias(x.lower()) for x in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing userID set as empty string\n",
    "df_clean = df_clean.filter(df_clean[\"userid\"] != \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.` We had some rows storing userid as empty string. This would break some operations carried out later and such records do not bring\n",
    "any value, therefore we can safely remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define udf to convert epoch timestamp to human readable date format\n",
    "convert_datetime = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new col event_timedate\n",
    "df_clean = df_clean.withColumn(\"event_timedate\", convert_datetime(df_clean.ts))\n",
    "\n",
    "# create new col registration_date\n",
    "df_clean = df_clean.withColumn(\"registration_timedate\", convert_datetime(df_clean.registration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.` new columns created properly showing up in schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast data type to sort our data\n",
    "df_clean = df_clean.withColumn(\"userid\",df_clean.userid.cast(IntegerType()))\n",
    "df_clean = df_clean.withColumn(\"ts\",df_clean.ts.cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort events by user and time\n",
    "df_clean = df_clean.sort(['userid','ts'], ascending=True)\n",
    "\n",
    "# check is events are sorted properly\n",
    "df_clean.filter(df_clean[\"userid\"] == 30).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.` We are sorting all users interactions in chronological order (asc). let's see if we can later extract some feature out of it. Having the ordered sequences of actions performed might help us to identify possible pain points churn users came across."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"eda\">3. Exploratory data analysis</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify churned users\n",
    "churn_ids = df_clean.select(\"userid\").where(df_clean.page ==\"Cancellation Confirmation\")\n",
    "\n",
    "churn_ids_list = churn_ids.select(\"userid\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new col to identify actions made by churned/not churned users\n",
    "df_clean = df_clean.withColumn(\"has_cancelled\",when((df_clean.userid.isin(churn_ids_list)), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for num of churned users\n",
    "df_clean.groupBy(\"has_cancelled\").agg(countDistinct('userid')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.` all rows/interactions made to churned users will return `has_cancelled ==1`\n",
    "\n",
    "`.` we only have 52 users who churned, even thought they count for 30% of our users base, this number might be too small, let's also consider to add users who downgraded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify users who downgraded\n",
    "downgrade_ids = df_clean.select(\"userid\").where(df_clean.page ==\"Downgrade\")\n",
    "\n",
    "downgrade_ids_list = downgrade_ids.select(\"userid\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add downgraded users col to dataset\n",
    "\n",
    "df_clean = df_clean.withColumn(\"has_downgraded\",when((df_clean.userid.isin(downgrade_ids_list)), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for num of downgraded users\n",
    "df_clean.groupBy(\"has_downgraded\").agg(countDistinct('userid')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.` all rows/interactions made to users who downgraded will return `has_downgraded ==1`\n",
    "\n",
    "`.` it will be interesting to look at if users who churned did downgrade before. It might be a sign user is about to leave.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for numerical variables\n",
    "df_clean.select([col[0] for col in df_clean.dtypes if col[1] != 'string']).toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.` we don't have too many insightful numerical variables.\n",
    "\n",
    "`.` `iteminsession` since its attached to sessionid, counter probably resets for every new session (i.e cold-start problem?)\n",
    "\n",
    "`.` `length` being the time (secs) a song was played.\n",
    "\n",
    "`.` other vars are not being of interest for our model, we will have to make sure to do some feature engineering work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create view 'sparkify_users_interactions_churn' to use sql\n",
    "\n",
    "df_clean.createOrReplaceTempView(\"sparkify_users_interactions_churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz iteminsession\n",
    "q1 = spark.sql('''\n",
    "        SELECT iteminsession\n",
    "        FROM sparkify_users_interactions_churn\n",
    "        ''').toPandas()\n",
    "\n",
    "\n",
    "sns.boxplot(q1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.` As expected from summary statistics displayed above, distribution of our variable is positively skewed to the right with an average \n",
    "of 115 iteminsession but still many users located under value reported as first quartile (27).\n",
    "\n",
    "`.` There are also \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column registering diff in days since user signed up\n",
    "df_clean = df_clean.withColumn(\"num_days_since_signup\", datediff(col(\"event_timedate\"),col(\"registration_timedate\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update table with new col\n",
    "df_clean.createOrReplaceTempView(\"sparkify_users_interactions_churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = spark.sql('''\n",
    "        SELECT DISTINCT userid, MAX(num_days_since_signup) as max_num_days_since_signup\n",
    "        FROM sparkify_users_interactions_churn\n",
    "        GROUP BY userid\n",
    "        \n",
    "        ''').toPandas()\n",
    "\n",
    "sns.distplot(q2.max_num_days_since_signup);\n",
    "\n",
    "plt.title('Distribution of users per last activity since registration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "q3 = spark.sql('''\n",
    "        SELECT gender,has_cancelled as has_churned, userid\n",
    "        FROM sparkify_users_interactions_churn\n",
    "    \n",
    "    group by 1,2,3\n",
    "''').toPandas()\n",
    "\n",
    "\n",
    "sns.countplot(x = 'gender', hue = 'has_churned', data = q3, palette = 'magma')\n",
    "plt.title('Distribution churned users by gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4 = spark.sql('''\n",
    "        SELECT level,has_cancelled as has_churned, userid\n",
    "        FROM sparkify_users_interactions_churn\n",
    "    group by 1,2,3\n",
    "''').toPandas()\n",
    "\n",
    "#sns.distplot(q1.has_churned);\n",
    "\n",
    "sns.countplot(x = 'level', hue = 'has_churned', data = q4, palette = 'magma')\n",
    "plt.title('Distribution of users by subscription type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of positive & negative interactions for churn\n",
    "q5 = spark.sql('''\n",
    "        SELECT has_cancelled as has_churned\n",
    "                ,(count(case when page in ('Add Friend','Add to Playlist','NextSong','Thumbs Up') then 1 else null end)/ count(*)) as perc_positive_interactions\n",
    "\n",
    "        FROM sparkify_users_interactions_churn\n",
    "        --where has_cancelled = 1\n",
    "     group by 1\n",
    "\n",
    "''').toPandas()\n",
    "\n",
    "\n",
    "sns.barplot(x = 'has_churned',y='perc_positive_interactions', data = q5, palette = 'magma')\n",
    "plt.title('Percentage positive interactions')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.` Not a real significant difference between the two groups of events.We should probably dig deeper and look at each one of them individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"feateng\">4. Features engineering</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- iteminsession: long (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionid: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: integer (nullable = true)\n",
      " |-- useragent: string (nullable = true)\n",
      " |-- userid: integer (nullable = true)\n",
      " |-- event_timedate: string (nullable = true)\n",
      " |-- registration_timedate: string (nullable = true)\n",
      " |-- has_cancelled: integer (nullable = false)\n",
      " |-- has_downgraded: integer (nullable = false)\n",
      " |-- num_days_since_signup: integer (nullable = true)\n",
      " |-- gender_index: double (nullable = false)\n",
      " |-- level_index: double (nullable = false)\n",
      " |-- auth_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "artist -> to see.\n",
    "\n",
    "auth -> keep -> transformed-> done\n",
    "#firstname -> remove\n",
    "gender -> keep -> transformed -> done\n",
    "iteminsession -> see what is it first\n",
    "#lastname -> remove\n",
    "length -> to remove ?\n",
    "level -> as cateforical -> done\n",
    "#location -> to remove\n",
    "#method -> to remove\n",
    "last event -> to create. before cancellation\n",
    "song -> to see\n",
    "#user agant -> remove\n",
    "first event -> to create\n",
    "vector with all events?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add meaningful events \n",
    "\n",
    "positive_experience = ['Add Friend','Add to Playlist','NextSong','Thumbs Up']\n",
    "negative_experience = ['Error','Submit Downgrade','Thumbs Down','Help']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "SI_gender = StringIndexer(inputCol='gender',outputCol='gender_index')\n",
    "SI_level = StringIndexer(inputCol='level',outputCol='level_index')\n",
    "SI_auth = StringIndexer(inputCol='auth',outputCol='auth_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Output column gender_index already exists.'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o2736.fit.\n: java.lang.IllegalArgumentException: requirement failed: Output column gender_index already exists.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.feature.StringIndexerBase$class.validateAndTransformSchema(StringIndexer.scala:91)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:109)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:152)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:135)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:109)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-54-a32e1d19a211>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf_clean\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSI_gender\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_clean\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_clean\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdf_clean\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSI_level\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_clean\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_clean\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mdf_clean\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSI_auth\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_clean\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_clean\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    130\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    131\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 132\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    133\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    134\u001B[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001B[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    294\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 295\u001B[0;31m         \u001B[0mjava_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    296\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    297\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_copyValues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    290\u001B[0m         \"\"\"\n\u001B[1;32m    291\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 292\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    293\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    294\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1255\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1256\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1257\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1258\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1259\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m     77\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mQueryExecutionException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m': '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstackTrace\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     78\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'java.lang.IllegalArgumentException: '\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 79\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mIllegalArgumentException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m': '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstackTrace\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     80\u001B[0m             \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     81\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIllegalArgumentException\u001B[0m: 'requirement failed: Output column gender_index already exists.'"
     ]
    }
   ],
   "source": [
    "df_clean = SI_gender.fit(df_clean).transform(df_clean)\n",
    "df_clean = SI_level.fit(df_clean).transform(df_clean)\n",
    "df_clean = SI_auth.fit(df_clean).transform(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaObject.__init__.<locals>.<lambda> at 0x7f1e7cc5c268>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1292, in <lambda>\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstname</th>\n",
       "      <th>gender</th>\n",
       "      <th>iteminsession</th>\n",
       "      <th>lastname</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>...</th>\n",
       "      <th>useragent</th>\n",
       "      <th>userid</th>\n",
       "      <th>event_timedate</th>\n",
       "      <th>registration_timedate</th>\n",
       "      <th>has_cancelled</th>\n",
       "      <th>has_downgraded</th>\n",
       "      <th>num_days_since_signup</th>\n",
       "      <th>gender_index</th>\n",
       "      <th>level_index</th>\n",
       "      <th>auth_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Offspring</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Natalee</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>Charles</td>\n",
       "      <td>168.30649</td>\n",
       "      <td>paid</td>\n",
       "      <td>Raleigh, NC</td>\n",
       "      <td>PUT</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-10-27 16:44:43</td>\n",
       "      <td>2018-09-13 00:49:30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Slakah the Beatchild Feat. Drake</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Natalee</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>Charles</td>\n",
       "      <td>227.00363</td>\n",
       "      <td>paid</td>\n",
       "      <td>Raleigh, NC</td>\n",
       "      <td>PUT</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-10-27 16:47:31</td>\n",
       "      <td>2018-09-13 00:49:30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paramore</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Natalee</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>Charles</td>\n",
       "      <td>209.42322</td>\n",
       "      <td>paid</td>\n",
       "      <td>Raleigh, NC</td>\n",
       "      <td>PUT</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-10-27 16:51:18</td>\n",
       "      <td>2018-09-13 00:49:30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ManÃÂ¡</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Natalee</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>Charles</td>\n",
       "      <td>307.72200</td>\n",
       "      <td>paid</td>\n",
       "      <td>Raleigh, NC</td>\n",
       "      <td>PUT</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-10-27 16:54:47</td>\n",
       "      <td>2018-09-13 00:49:30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Natalee</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>Charles</td>\n",
       "      <td>238.99383</td>\n",
       "      <td>paid</td>\n",
       "      <td>Raleigh, NC</td>\n",
       "      <td>PUT</td>\n",
       "      <td>...</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-10-27 16:59:54</td>\n",
       "      <td>2018-09-13 00:49:30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             artist       auth firstname gender  \\\n",
       "0                     The Offspring  Logged In   Natalee      F   \n",
       "1  Slakah the Beatchild Feat. Drake  Logged In   Natalee      F   \n",
       "2                          Paramore  Logged In   Natalee      F   \n",
       "3                           ManÃÂ¡  Logged In   Natalee      F   \n",
       "4                      Taylor Swift  Logged In   Natalee      F   \n",
       "\n",
       "   iteminsession lastname     length level     location method    ...      \\\n",
       "0              0  Charles  168.30649  paid  Raleigh, NC    PUT    ...       \n",
       "1              1  Charles  227.00363  paid  Raleigh, NC    PUT    ...       \n",
       "2              2  Charles  209.42322  paid  Raleigh, NC    PUT    ...       \n",
       "3              3  Charles  307.72200  paid  Raleigh, NC    PUT    ...       \n",
       "4              4  Charles  238.99383  paid  Raleigh, NC    PUT    ...       \n",
       "\n",
       "                                           useragent  userid  \\\n",
       "0  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...       2   \n",
       "1  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...       2   \n",
       "2  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...       2   \n",
       "3  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...       2   \n",
       "4  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...       2   \n",
       "\n",
       "        event_timedate registration_timedate  has_cancelled  has_downgraded  \\\n",
       "0  2018-10-27 16:44:43   2018-09-13 00:49:30              0               1   \n",
       "1  2018-10-27 16:47:31   2018-09-13 00:49:30              0               1   \n",
       "2  2018-10-27 16:51:18   2018-09-13 00:49:30              0               1   \n",
       "3  2018-10-27 16:54:47   2018-09-13 00:49:30              0               1   \n",
       "4  2018-10-27 16:59:54   2018-09-13 00:49:30              0               1   \n",
       "\n",
       "  num_days_since_signup  gender_index level_index auth_index  \n",
       "0                    44           0.0         0.0        0.0  \n",
       "1                    44           0.0         0.0        0.0  \n",
       "2                    44           0.0         0.0        0.0  \n",
       "3                    44           0.0         0.0        0.0  \n",
       "4                    44           0.0         0.0        0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = df_clean.toPandas()\n",
    "\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KEEP THIS ONE FOR THE VERY END\n",
    "#drop unnecessary cols\n",
    "#cols = [\"firstname\",\"lastname\",\"location\",\"method\",\"useragent\",\"status\",\"registration\"]\n",
    "\n",
    "#add ts, length, at the end\n",
    "#df_clean = df_clean.drop(*cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"model\">5. Modeling</a>\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update table with added features\n",
    "df_clean.createOrReplaceTempView(\"sparkify_users_interactions_churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = spark.sql('''\n",
    "        SELECT\n",
    "        userid,\n",
    "        gender_index,\n",
    "        has_cancelled as label,\n",
    "        max(level_index) as max_level_index,\n",
    "        max(auth_index) as max_auth_index,\n",
    "        max(num_days_since_signup) as max_num_days_since_signup\n",
    "        ,count(distinct sessionid) as num_sessions\n",
    "        ,count(*) as num_total_interactions\n",
    "        ,count(case when page in ('Add Friend','Add to Playlist','NextSong','Thumbs Up') then 1 else null end) as num_positive_interactions\n",
    "        ,count(case when page in ('Error','Submit Downgrade','Thumbs Down','Help') then 1 else null end) as num_negative_interactions\n",
    "        FROM sparkify_users_interactions_churn\n",
    "    group by 1,2,3\n",
    "    order by 1\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.drop('userid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Features as Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.` Create a vector from the combined columns of all of the features, see [here](https://spark.apache.org/docs/1.4.1/ml-features.html#vectorassembler)\n",
    "\n",
    "VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. \n",
    "In each row, the values of the input columns will be concatenated into a vector in the specified order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = [\"gender_index\", \"max_level_index\", \"max_auth_index\",\"max_num_days_since_signup\",\"num_sessions\",\"num_total_interactions\",\"num_positive_interactions\",\"num_negative_interactions\"],outputCol = \"features_vec\")\n",
    "\n",
    "features_df = assembler.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Features Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.`StandardScaler transforms a dataset of Vector rows, see [here](https://spark.apache.org/docs/1.4.1/ml-features.html#standardscaler)\n",
    "                                                                  \n",
    "Normalizing each feature to have unit standard deviation and/or zero mean.\n",
    "This can be relevant to our model since we are dealing with features of different scale and do not necessarly want highest/lowest values to overweight in the model predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"vec_features\", outputCol=\"features\", withStd=True)\n",
    "\n",
    "scaler_model = scaler.fit(features_df)\n",
    "\n",
    "features_df = scaler_model.transform(features_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, valid = features_df.randomSplit([0.6, 0.2, 0.2], seed = 42)\n",
    "print(\"size training :\" + str(train.count()))\n",
    "print(\"size test\" + str(test.count()))\n",
    "print(\"size validation:\" + str(valid.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "rfc = RandomForestClassifier(featuresCol = 'features', labelCol = 'label', seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [lr,rfc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol = 'label', predictionCol='prediction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-50-aa270faf22de>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmodel_list\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0;31m# get model name\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mmodel_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;31m# print training started\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model_list' is not defined"
     ]
    }
   ],
   "source": [
    "for model in model_list:\n",
    "    # get model name \n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    print(model_name, 'training started')\n",
    "    \n",
    "    # start time\n",
    "    start = time.time()\n",
    "    # fit the models on train\n",
    "    model = model.fit(train)\n",
    "    # end time\n",
    "    end = time.time()\n",
    "     \n",
    "    # print training completed\n",
    "    print(model_name, 'training success')\n",
    "\n",
    "    # print exec time\n",
    "    print('Time taken for {} is:'.format(model_name),(end-start),'seconds')\n",
    "    \n",
    "    # predict\n",
    "    print(model_name, 'predicting started')\n",
    "    predictions = model.transform(valid)\n",
    "    print(model_name, 'predicting ended')\n",
    "    \n",
    "    # evaluation metrics\n",
    "    print('F1 for {} is:'.format(model_name), evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))\n",
    "\n",
    "    accuracy = predictions.filter(predictions.label == predictions.prediction).count() / (predictions.count())\n",
    "    print(\"The accuracy of the {} model is:\".format(model_name), accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"results\">6. Conclusion</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}